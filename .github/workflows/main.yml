name: Scrape Field Statuses

on:
  schedule:
    # Runs every hour at the top of the hour
    # You can adjust this cron schedule if needed
    # For example, '*/30 * * * *' would be every 30 minutes
    # '0 6,12,18 * * *' would be at 6 AM, 12 PM, and 6 PM UTC
    - cron: '0 * * * *'
  workflow_dispatch: # Allows you to run this workflow manually from the Actions tab

jobs:
  scrape_and_commit:
    runs-on: ubuntu-latest # Use the latest stable version of Ubuntu Linux as the runner

    steps:
      # Step 1: Check out your repository code
      - name: Check out repository
        uses: actions/checkout@v4

      # Step 2: Set up Node.js environment
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20.x' # Use a Node.js version (e.g., 20.x, an LTS version)

      # Step 3: Install project dependencies
      # 'npm ci' is generally recommended for CI environments as it uses package-lock.json
      # for faster and more reliable installs than 'npm install'
      - name: Install dependencies
        run: npm ci

      # Step 4: Run your scraper script
      # This will generate/update the field_statuses.json file
      - name: Run scraper script
        run: node scraper.js

      # Step 5: Commit and push the updated field_statuses.json if it changed
      - name: Commit and push if data changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: Automate: Update field_statuses.json
          file_pattern: field_statuses.json # Only commit if this file changes
          # Optional: Add commit author and email if needed
          # commit_user_name: My GitHub Actions Bot
          # commit_user_email: my-github-actions-bot@example.com
          # commit_options: '--no-verify --signoff'
